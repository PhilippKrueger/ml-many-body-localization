\documentclass[reprint,amsmath,amssymb,aps,prb]{revtex4-2}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{hyperref}% add hypertext capabilities
\usepackage{xcolor}

%% Philipps stuff %%
\usepackage{subcaption}
\captionsetup[subfigure]{list=true, font=large, labelfont=bf, 
	labelformat=brace, position=top}


%% Code stuff %%
\usepackage{listings} % insert code fragments

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\begin{document}

%\include{guidelines}

\title{Machine Learning of Many Body Localization}

\author{Philipp Krüger}

\date{\today}% It is always \today, today,
             %  but any date may be explicitly specified

\begin{abstract}
Exact diagonalization was used to find the reduced density matrices of the lowest energy eigenstate of the Heisenberg Model with an additional field in z-direction at low and high disorder strength. The resulting dataset representing extended and localized phases was used to train a neural network. Afterwards, the trained network was applied on intermediate disorder strengths to deduct the critical disorder strength for a phase transition. The phase transition occured for all system sizes at $W_c = J$. 
%todo W-c is the same for all system sizes?
\end{abstract}


\maketitle

\section{Introduction}

First, the physical model is introduced. Secondly, the concept of exact diagonalization is briefly presented. As we use the reduced density matrices as the feature for the neural network, we state briefly their derivation.

Review Literature on task. How other people find $W_c$?

Why is the topic interesting? => Finding Wc?

\subsection{Hamiltonian of the Heisenberg model}

What is it used for? With $J=1$, $h_i \in \left[-W, W\right]$

\begin{equation}
	H=\underbrace{J\sum_i \vec{S}_i\cdot\vec{S}_{i+1}}_{\text{Exchange Energy}}-\underbrace{\sum_ih_iS_i^z}_{\text{Random Field}}
\end{equation}

Outcome expectation: 

In the ergodic phase (delocalized phase) $h<h_c$, the many-body eigenstates
are thermal,20–23 so the isolated quantum system can relax to
thermal equilibrium under the dynamics due to its Hamiltonian.
In the thermodynamic limit $L\rightarrow\infty$, the system thus
successfully serves as its own heat bath in the ergodic phase.
In a thermal eigenstate, the reduced density operator of a
finite subsystem converges to the equilibrium thermal distribution
for $L\rightarrow\infty$. Thus the entanglement entropy between a
finite subsystem and the remainder of the system is, for  $L\rightarrow\infty$, the thermal equilibrium entropy of the subsystem. At
nonzero temperature, this entanglement entropy is extensive,
proportional to the number of degrees of freedom in the subsystem.


In the many-body localized phase $h>h_c$, on the other
hand, the many-body eigenstates are not thermal:2 the
“eigenstate-thermalization hypothesis”20–23 is false in the
localized phase. Thus in the localized phase, the isolated
quantum system does not relax to thermal equilibrium
under the dynamics of its Hamiltonian. The infinite system
fails to be a heat bath that can equilibrate itself. It is a
“glass” whose local configurations at all times are set by the
initial conditions. Here the eigenstates do not have extensive
entanglement, making them accessible to density-matrixrenormalization-group–type
numerical techniques.5 A limit
of the localized phase that is simple is J= 0 with $h>0$. Here
the spins do not interact, all that happens dynamically is
local Larmor precession of the spins about their localrandom
fields. No transport of energy or spin happens and
the many-body eigenstates are simply product states with
each spin either “up” or “down.”
\url{https://doi.org/10.1103/PhysRevB.82.174411}


non-thermalising phase, in which the system violates the Eigenstate Thermalisation hypothesis
\url{https://arxiv.org/pdf/1610.03042.pdf}


J<g: (large W) localized phase (adiabatically connected to trivial state), J>g extended phase (small W) (ordered??)

Is scaling important?

\subsection{Exact Diagonalization}

Introduce concepts: Exact Diagonalization, 

\subsection{Areal (reduced??) Density Matrix}

areal Density Matrix:
\url{http://www.thphys.nuim.ie/staff/jvala/Lecture_9.pdf}

\begin{figure}[h!]
	\includegraphics[width=0.99\linewidth]{figures/reduced_density_matrix.jpg}
	\caption{Example of a figure~\cite{Orus2013}.}
	\label{fig:adm}
\end{figure}

vs

direct density matrix:
(DOI: 10.1103/PhysRevB.99.054208 says 
Instead of dividing the system into two subsystems A
and B to calculate the reduced density matrix of an eigenstate rho a and using the entanglement spectrum as the training data set 34,35, we directly feed the
probability density of the eigenstate psi i computed in the
spin basis to the machines as the training data set. The
reason for doing so is that, although by preprocessing
the training data can reduce the dimension and filter out
redundant information, useful information contained in
the wavefunction of the entire system can also be lost.)

Conclusion: => areal

\subsection{Neural Networks and Convolutional Neural Networks}

Neural Network, CNN





\section{Materials and Methods}%todo: Action title

\subsection{Code Protocol}

\begin{enumerate}
	\item Function: Generate random disorder strength using a uniform distribution
	\item Function: Generate Hamiltonian from disorder strength and system size
	\item Define and understand phase transition from extended phase to localized phase
	\item Function: Picks a number M of lowest eigenstates near Energy E = 0
	\item Function: Generate density matrix for an eigenstate
	\item Function: Visualize density matrices
	\item  Function: Set up machine learning model that takes density matrices of different W as an input, 
	and predicts whether the state represents an extended or a localized phase.
	\item Function: Make predictions for different system sizes L and block sizes n.
	\item Function: Plot the predicitons over W.
	\item Function: Extract $W_c$
	from the data.
\end{enumerate}


Explain Flow with figure

\begin{figure}[h!]
	\includegraphics[width=0.99\linewidth]{figures/flowchart.jpg}
	\caption{Example of a figure~\cite{Orus2013}.}
	\label{fig:example}
\end{figure}

Fig.~\ref{fig:example}

\subsection{Machine Learning Models and Error Metrics}

Two different approaches:

regression like: Optimizer RMSprop, Loss MAE

classification focused: Optimizer Adam

Loss: BinaryCrossentropy from \url{https://keras.io/api/losses/probabilistic_losses/#binary_crossentropy-function}:
Computes the cross-entropy loss between true labels and predicted labels.

Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For each example, there should be a single floating-point value per prediction.

Optimizer: Stochastic Gradient Descent, Learning rate? Formula? Principle?

Explain metrics and errors and why they are used. Which ml models are used and why?

Use drop out to reduce overfitting. Low sample size techniques:

\section{Results}%todo: Action title

\subsection{Generation of density matrix training set}

Chosen parameters: $L \in \{10, 11, 12\}$. Repetitions: 500. Measure of variation in the test set??

\newpage
\begin{figure*}[h!]
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N10_trainingset_groundstate_Wmax0.5.pdf}
		\subcaption{Ergodic phase L = 10.}
	\end{subfigure}
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N11_trainingset_groundstate_Wmax0.5.pdf}
		\subcaption{Ergodic phase L = 11.}
	\end{subfigure}
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N12_trainingset_groundstate_Wmax0.5.pdf}
		\subcaption{Ergodic phase L = 12.}
	\end{subfigure}
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N10_trainingset_groundstate_Wmax8.0.pdf}
		\subcaption{Localized phase L = 10.}
	\end{subfigure}
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N11_trainingset_groundstate_Wmax8.0.pdf}
		\subcaption{Localized phase L = 11.}
	\end{subfigure}
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N12_trainingset_groundstate_Wmax8.0.pdf}
		\subcaption{Localized phase L = 12.}
	\end{subfigure}
	\caption{Real part of the density matrix of an ergodic/localized phase for different system sizes L.}
\end{figure*}


%\begin{figure}
%	\begin{subfigure}[c]{0.2\textwidth}
%			\includegraphics[width=\textwidth]{../results/N11_trainingset_groundstate_Wmax0.5.jpg}
%		\subcaption{}
%	\end{subfigure}
%	\begin{subfigure}[c]{0.2\textwidth}
%		\includegraphics[width=\textwidth]{../results/N11_trainingset_groundstate_Wmax8.0.jpg}
%		\subcaption{}
%	\end{subfigure}
%	\caption{Zwei Bilder mit Subfigure nebeneinander}
%\end{figure}


Plots: 
What is computationally realizable in 1h concerning time?
The training set was sufficiently large enough 

We only need M Eigenstates

This is how corresponding density matrices look like

This will be our parameter space for n, L

\subsection{Prediction of extended vs localized phase}

Training and validation scores:

\newpage
\begin{figure*}[h!]
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N10_accuracy_loss_epochs}
		\subcaption{$N=10$}
		\label{fig:N10_accuracy_loss_epochs}
	\end{subfigure}
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N11_accuracy_loss_epochs}
		\subcaption{$N=11$}
		\label{fig:N11_loss_epochs}
	\end{subfigure}
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N12_accuracy_loss_epochs}
		\subcaption{$N=12$}
		\label{fig:N12_loss_epochs}
	\end{subfigure}
	\caption{Accuracy and loss of neuronal network plotted over training epochs.}
\end{figure*}



\subsection{$W_c$ analysis}


Now we generate testing set with $W_{max} \in \left[0,4\right]$. We suspect $W_c$ to be at 1 ??%todo
We fit a logistic curve and extract $W_c$ as a parameter.

\newpage
\begin{figure*}[h!]
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N10_predict_wc}
		\subcaption{$N=10$}
		\label{fig:N10_predict_wc}
	\end{subfigure}
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N11_predict_wc}
		\subcaption{$N=11$}
		\label{fig:N11_predict_wc}
	\end{subfigure}
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N12_predict_wc}
		\subcaption{$N=12$}
		\label{fig:N12_predict_wc}
	\end{subfigure}
	\caption{Phase prediction with localized and ergodic phase defined as 1, 0.}
\end{figure*}


These are our $W_c$ depending on n, L.

%todo: Plot with extracted $W_Cs$

\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{../results/Wc_L_dependency}
	\caption{}
	\label{fig:Wc_L_dependency}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{../results/Wc_N_dependency}
\caption{}
\label{fig:Wc_N_dependency}
\end{figure}


\section{Conclusion}%todo: Action title

$W_c$ depends on n, L (yes/no).

$W_c$ prediction coincides with the expectation (yes/no)

$W_c$ is dependent on these and that effects => scaling analysis? (yes/no)

Citations are numerical\cite{epr}, some more citations~\cite{feyn54,Bire82,Berman1983,witten2001,Davies1998}. 


\bibliography{bibsamp}% Produces the bibliography via BibTeX.


\appendix


\begin{widetext}
\section{Code listing} \label{app:codes}
Please copy your code in the appendix.
\begin{lstlisting}[language=Python]
"""

Description

"""

import numpy as np

code
\end{lstlisting}
\end{widetext}



\end{document}
