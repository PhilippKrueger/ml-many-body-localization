\documentclass[reprint,amsmath,amssymb,aps,prb]{revtex4-2}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{hyperref}% add hypertext capabilities
\usepackage{xcolor}
\usepackage{braket}
\usepackage[english]{babel}

%% Philipps stuff %%
\usepackage{subcaption}
\captionsetup[subfigure]{list=true, font=large, labelfont=bf, 
	labelformat=brace, position=top}


%% Code stuff %%
\usepackage{listings} % insert code fragments

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\begin{document}

%\include{guidelines}

\title{Machine Learning of Many Body Localization}

\author{Philipp Krüger}

\date{\today}% It is always \today, today,
             %  but any date may be explicitly specified

\begin{abstract}
Exact diagonalization was used to find the reduced density matrices for different block sizes of the lowest energy eigenstate of the Heisenberg Model with an additional random field in z-direction at low and high disorder strength. The resulting dataset representing extended and localized phases was used to train a neural network. Afterwards, the trained network was applied on intermediate disorder strengths to deduct the critical disorder strength for a phase transition. The phase transition occurred for all system sizes at $W_c = J$. %todo system size? Block size? ML model performance and Wc dependency
\end{abstract}

\maketitle

\section{Introduction}

Firstly, the physical model is introduced. Secondly, the concept of exact diagonalization is briefly presented. As we use the reduced density matrices as the feature for the neural network, we state briefly their derivation.
%Review Literature on task. How other people find $W_c$?
%Why is the topic interesting? => Finding Wc?
\subsection{Physical model}

\subsubsection{Hamiltonian of the Heisenberg model}

The hamiltonian of the Heisenberg model is shown in equation \ref{hamiltonian}. In the course of further analysis, we choose $J=1$ and sample $h$ from a uniform distribution such that $h_i \in \left[-W, W\right]$.

\begin{equation}
	H=\underbrace{J\sum_i \vec{S}_i\cdot\vec{S}_{i+1}}_{\text{Exchange Energy}}-\underbrace{\sum_ih_iS_i^z}_{\text{Random Field}}\label{hamiltonian}
\end{equation}

\subsubsection{Expectations for the ground state}

The expectation for the ground state is dependent on the ratio of the coupling and the local random field. 

For $\frac{W}{J} \ll 1$, we expect an extended, delocalized phase, since the exchange energy dominates over the small external field. Therefore, the system can relax to thermal equilibrium serving as its own heat bath in the limit of large system size $L\rightarrow\infty$.
Here, the reduced density operator of a finite subsystem converges to the equilibrium thermal distribution
for $L\rightarrow\infty$.\cite{Pal2010}

For $\frac{W}{J} \gg 1$, we can expect a localized phase, since the $h_i$ factors dominate over the exchange energy. The resulting states are expected to be product states of spins "up" or "down", as the external field points in z-direction. Also an infinite system cannot equilibrate itself. The local configurations are set by the initial conditions at all times and are adiabatically connected to the trivial state.\cite{Pal2010}

%non-thermalising phase, in which the system violates the Eigenstate Thermalisation hypothesis
%\url{https://arxiv.org/pdf/1610.03042.pdf}
%J<g: (large W) localized phase (), 

\subsection{Exact diagonalization}

Exact diagonalization (ED) is a numerical technique we can use to solve the Schrödinger Equation $H\ket{\psi}=E\ket{\psi}$ for the eigenvalues $E$ and eigenvectors $\ket{\psi}$. This only works of the Hamiltonian $H$ represents a discrete and finite system. Most quantum many-particle problems lead to a sparse matrix representation of the Hamiltonian, where only a very small fraction of the matrix
elements is non-zero.\cite{Weisse2008} An efficient method to find ground states is the Lanczos algorithm.\cite{Lanczos1950} At first, the algorithm was numerically unstable. This issue was overcome in 1970 by Ojalvo and Newman.\cite{Ojalvo1970}

\subsection{Reduced Density Matrix}

The usefulness of reduced density matrices has already been shown by White in 1992 with ground states of Heisenberg chains \cite{White1992}. In our case we use areal density matrices as features for the neural network to predict the critical disorder strength of a phase change from delocalized to localized. The reduced density matrix is defined in equation \ref{red_density}. Physically, the reduced density matrix $\rho_A$, provides correct measurement statistics for subsystem A.

\begin{eqnarray}
\rho_{AB}&=&\ket{\psi_A}\bra{\psi_A}\otimes\ket{\psi_B}\bra{\psi_B}\\
\rho_A&=&\text{Tr}_B(\rho_{AB})=\ket{\psi_A}\bra{\psi_A}\text{Tr}\left(\ket{\psi_B}\bra{\psi_B}\right)\label{red_density}
\end{eqnarray}%\url{http://www.thphys.nuim.ie/staff/jvala/Lecture_9.pdf}
%\begin{figure}[h!]
%	\includegraphics[width=0.99\linewidth]{figures/reduced_density_matrix.jpg}
%	\caption{Example of a figure~\cite{Orus2013}.}
%	\label{fig:adm}
%\end{figure}

The reduced density matrix was also used by Zhang in 2019 to learn the localization transition in disordered quantum Ising spin chains. Here, the motivation was to reduce the dimension and filter out redundant information. However, it proved to be inferior in comparison to the full density matrix in the analysis. \cite{Zhang2019}


\subsection{Neural Networks and Convolutional Neural Networks}

%Explain Principle, usage, why can we use it here?

Nice, since we can label our dataset with parameters, where we know the outcome.
Image classification is a can be solved efficiently with machine learning.%todo: Cite stuff


\section{Computational Methods}

The strategy for implementation was as follows:

\begin{enumerate}
	\item Generate Hamiltonian from random disorder strength and system size and calculate lowest eigenstate near Energy $E = 0$.
	\item Generate density matrix from the eigenstate and the respective reduced density matrices for defined block sizes $n$.
	\item  Set up machine learning model per n, L that takes density matrices of different W as an input and predicts whether the state represents an extended or a localized phase.
	\item Make predictions for different system sizes L and block sizes n and plot the predicitons over W. Then extract $W_c$	from the data by using a fit function.
\end{enumerate}

Critical decisions and specifications for each steps are listed below.

\subsubsection{Eigenvalue solver}

scipy.sparse.linalg.eigsh(A, k=6, sigma=0, which='SM', ) pretty slow for N=10 : 70s
=> qutip.Qobj(H).groundstate() only 0.7s

sigma = 0:%todo: really needed?
which='SM': smallest magnitude.

Eigenvalue solver, stability convergence

Result for 2 samples each:
Training Set N=9 completed after 0.9669864177703857 seconds.
Training Set N=10 completed after 5.634985685348511 seconds.
Training Set N=11 completed after 25.606987714767456 seconds.
Training Set N=12 completed after 139.2209861278534 seconds.

\subsubsection{Density matrix reducer}

The partial trace is an operation that reduces the dimension of a Hilbert space by eliminating some degrees of freedom by averaging (tracing). In this sense it is therefore the converse of the tensor product. It is useful when one is interested in only a part of a coupled quantum system. For open quantum systems, this typically involves tracing over the environment leaving only the system of interest. In QuTiP the class method qutip.Qobj.ptrace is used to take partial traces. qutip.Qobj.ptrace acts on the qutip.Qobj instance for which it is called, and it takes one argument sel, which is a list of integers that mark the component systems that should be kept. All other components are traced out.
\url{http://qutip.org/docs/3.1.0/guide/guide-tensor.html}

Density matrix reducer

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figures/partialtrace_proof_of_concept}
\caption{Proof of concept for partial trace calculation.}
\label{fig:partialtrace_proof_of_concept}
\end{figure}
%todo: reference qutip


\subsubsection{Machine Learning Models and Error Metrics}

A neural network was used. As the sample size was %todo,
a number of layers was selected to avoid overfitting.
Dropout for regularization.

Cite Low sample size techniques:

Prominent optimzer Adam was used which has these and that strengths.
For a binary classification problem, it is common to use as a loss binary crossentropy:

%todo: formula

Loss: BinaryCrossentropy from \url{https://keras.io/api/losses/probabilistic_losses/#binary_crossentropy-function}:
Computes the cross-entropy loss between true labels and predicted labels.

Reason:
Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For each example, there should be a single floating-point value per prediction.

\subsubsection{$W_c$ finder}


Fit with step like function: Candidates: logistic, heaviside

Choose logisitc nicer, because differentiable and transition is not abrupt?%todo




\section{Results}%todo: Action title

\subsection{Generation of density matrix training set}

Chosen parameters: $L \in \{10, 11, 12\}$. Repetitions: 500. Measure of variation in the test set??

\begin{figure*}[h!]
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N10_trainingset_groundstate_Wmax0.5.pdf}
		\subcaption{Ergodic phase L = 10.}
	\end{subfigure}
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N11_trainingset_groundstate_Wmax0.5.pdf}
		\subcaption{Ergodic phase L = 11.}
	\end{subfigure}
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N12_trainingset_groundstate_Wmax0.5.pdf}
		\subcaption{Ergodic phase L = 12.}
	\end{subfigure}
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N10_trainingset_groundstate_Wmax8.0.pdf}
		\subcaption{Localized phase L = 10.}
	\end{subfigure}
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N11_trainingset_groundstate_Wmax8.0.pdf}
		\subcaption{Localized phase L = 11.}
	\end{subfigure}
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N12_trainingset_groundstate_Wmax8.0.pdf}
		\subcaption{Localized phase L = 12.}
	\end{subfigure}
	\caption{Real part of the density matrix of an ergodic/localized phase for different system sizes L.}
\end{figure*}


%\begin{figure}
%	\begin{subfigure}[c]{0.2\textwidth}
%			\includegraphics[width=\textwidth]{../results/N11_trainingset_groundstate_Wmax0.5.jpg}
%		\subcaption{}
%	\end{subfigure}
%	\begin{subfigure}[c]{0.2\textwidth}
%		\includegraphics[width=\textwidth]{../results/N11_trainingset_groundstate_Wmax8.0.jpg}
%		\subcaption{}
%	\end{subfigure}
%	\caption{Zwei Bilder mit Subfigure nebeneinander}
%\end{figure}


Plots: 
What is computationally realizable in 1h concerning time?
The training set was sufficiently large enough 

We only need M Eigenstates

This is how corresponding density matrices look like

This will be our parameter space for n, L

\subsection{Prediction of extended vs localized phase}

Training and validation scores:

\newpage
\begin{figure*}[h!]
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N10_accuracy_loss_epochs}
		\subcaption{$N=10$}
		\label{fig:N10_accuracy_loss_epochs}
	\end{subfigure}
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N11_accuracy_loss_epochs}
		\subcaption{$N=11$}
		\label{fig:N11_loss_epochs}
	\end{subfigure}
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N12_accuracy_loss_epochs}
		\subcaption{$N=12$}
		\label{fig:N12_loss_epochs}
	\end{subfigure}
	\caption{Accuracy and loss of neuronal network plotted over training epochs.}
\end{figure*}



\subsection{$W_c$ analysis}


Now we generate testing set with $W_{max} \in \left[0,4\right]$. We suspect $W_c$ to be at 1 ??%todo
We fit a logistic curve and extract $W_c$ as a parameter.

\newpage
\begin{figure*}[h!]
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N10_predict_wc}
		\subcaption{$N=10$}
		\label{fig:N10_predict_wc}
	\end{subfigure}
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N11_predict_wc}
		\subcaption{$N=11$}
		\label{fig:N11_predict_wc}
	\end{subfigure}
	\begin{subfigure}[c]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../results/N12_predict_wc}
		\subcaption{$N=12$}
		\label{fig:N12_predict_wc}
	\end{subfigure}
	\caption{Phase prediction with localized and ergodic phase defined as 1, 0.}
\end{figure*}


These are our $W_c$ depending on n, L.

%todo: Plot with extracted $W_Cs$

\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{../results/Wc_L_dependency}
	\caption{}
	\label{fig:Wc_L_dependency}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{../results/Wc_N_dependency}
\caption{}
\label{fig:Wc_N_dependency}
\end{figure}


\section{Conclusion}%todo: Action title

$W_c$ depends on n, L (yes/no).

$W_c$ prediction coincides with the expectation (yes/no)

$W_c$ is dependent on these and that effects => scaling analysis? (yes/no)

Citations are numerical\cite{epr}, some more citations~\cite{feyn54,Bire82,Berman1983,witten2001,Davies1998}. 

\bibliography{zotero}
%\bibliography{bibsamp}% Produces the bibliography via BibTeX.


\appendix


\begin{widetext}
\section{Code listing} \label{app:codes}
Please copy your code in the appendix.
\begin{lstlisting}[language=Python]
"""

Description

"""

import numpy as np

code
\end{lstlisting}
\end{widetext}



\end{document}
