\documentclass[reprint,amsmath,amssymb,aps,prb]{revtex4-2}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{hyperref}% add hypertext capabilities
\usepackage{xcolor}
\usepackage{braket}
\usepackage[english]{babel}

%% Philipps stuff %%
\usepackage[font={small}]{caption}
\usepackage{subcaption}
\captionsetup[subfigure]{list=true, font=small, labelfont=bf, 
	labelformat=brace, position=top}
\usepackage{amsmath}
\def\arraystretch{1.5}
\usepackage{float}
\usepackage{lipsum}


%% Code stuff %%
\usepackage{listings} % insert code fragments

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\begin{document}

%\include{guidelines}

\title{Machine Learning of Many Body Localization}

\author{Philipp Krüger}

\date{\today}% It is always \today, today,
             %  but any date may be explicitly specified

\begin{abstract}
The goal of this study was to find the quantum phase transition at intermediate local disorder strengths on a Heisenberg chain. Exact diagonalization was used to find the reduced density matrices for a different number of consecutive spins for the lowest energy eigenstate of the Heisenberg model with an additional random field in z-direction at low and high disorder strengths. The resulting dataset representing extended and localized phases was used to train a neural network. Afterwards, the trained network was applied on intermediate disorder strengths to deduct the critical disorder strength for a phase transition. This phase transition was for all system sizes predicted to be around $W_c = 1.5 J$ for the system sizes $L\in\{9, 10, 11, 12\}$ and block sizes $n\in\left[1,6\right]$. Low block sizes suffered from a low accuracy in the machine learning model, whereas for higher block sizes the $W_c$ values approached $W_c=J$.  %todo system size? Block size? ML model performance and Wc dependency Finite size scaling?
\end{abstract}

\maketitle

\section{Introduction}

The physical model and the concept of exact diagonalization is presented first. As we use reduced density matrices as features for the neural network, we explain briefly their computation and meaning.
%Review Literature on task. How other people find $W_c$?
%Why is the topic interesting? => Finding Wc?
\subsection{Physical model}

\subsubsection{Hamiltonian of the Heisenberg model and physical expectation}

The Hamiltonian of the Heisenberg model is shown in equation \ref{hamiltonian}. In the course of further analysis, we choose $J=1$ and sample $h$ from a uniform distribution such that $h_i \in \left[-W, W\right]$.

\begin{equation}
	H=\underbrace{J\sum_i \vec{S}_i\cdot\vec{S}_{i+1}}_{\text{Exchange Energy}}-\underbrace{\sum_ih_iS_i^z}_{\text{Random Field}}\label{hamiltonian}
\end{equation}

The expectation for the ground state is dependent on the ratio of the coupling and the local random field. 

For $\frac{W}{J} \ll 1$, we expect an delocalized, extended phase, since the exchange energy dominates over the small external field. Therefore, the system can relax to thermal equilibrium serving as its own heat bath in the limit of large system size $L\rightarrow\infty$.
Here, the reduced density operator of a finite subsystem converges to the equilibrium thermal distribution
for $L\rightarrow\infty$.\cite{Pal2010}

For $\frac{W}{J} \gg 1$, we can expect a localized phase, since the $h_i$ factors dominate over the exchange energy. The resulting states are expected to be product states of spins "up" or "down", as the external field points in z-direction. Also an infinite system cannot equilibrate itself. The local configurations are set by the initial conditions at all times and are adiabatically connected to the trivial state.\cite{Pal2010}

\subsubsection{Characterization of ergodic and localized regimes by different metrics}

There are a few different ways of how to distinguish the ergodic and the localized regime by accessible metrics. 
One can study spectral analysis, where the ergodic states are distributed like a Gaussian Orthogonal Ensemble (GOE), and the localized states follow a Poisson distribution.\cite{Laumann2014}

Another interesting metric is the entaglement entropy, which indicates the information spread between different system parts\cite{Nandkishore_2015}. In the ergodic phase, the reduced density matrix $\rho_A$ of a ground state is expected to be thermal. This leads to a volume law scaling for the entanglement entropy\cite{Altman_2015}.	On the other hand, localized eigenstates show area-law scaling\cite{yu2019bulk}.
%W—>0 : reduced matrix : thermal density matrix , volume law entanglement
%W —> \infty : reduced density matrix: … , area law entanglement
%n is finite and large enough
%reduced density matrix would be different on two sides.

In this study, we solve the Hamiltonian via exact diagonalization to predict the phase change by training a neural network with low and high disorder strengths, assuming that the resulting reduced density matrices represent an ergodic and localized phase respectively. To access the ground states, previous approaches made use of the shift-invert method \cite{Luitz2015}.


%\begin{figure}
%	\centering
%	\includegraphics[width=1\linewidth]{figures/heisenberg_chain}
%	\caption{Visualization of the random disorder strength efftect on a ground state.}
%	\label{fig:heisenberg_chain}
%\end{figure}

\subsection{Exact diagonalization}

Exact diagonalization (ED) is a numerical technique we can use to solve the time independent Schrödinger Equation $H\ket{\psi}=E\ket{\psi}$ for the eigenvalues $E$ and eigenvectors $\ket{\psi}$. This only works of the Hamiltonian $H$ represents a discrete and finite system. Most quantum many-particle problems lead to a sparse matrix representation of the Hamiltonian, where only a very small fraction of the matrix
elements is non-zero.\cite{Weisse2008} An efficient method to find ground states is the Lanczos algorithm.\cite{Lanczos1950} At first, the algorithm was numerically unstable. This issue was overcome in 1970 by Ojalvo and Newman.\cite{Ojalvo1970} In this study, we rely on the Lanczos algorithm for the eigensolver.

\subsection{Reduced Density Matrix}

The usefulness of reduced density matrices has already been shown by White in 1992 with ground states of Heisenberg chains \cite{White1992}. In our case we use areal density matrices as features for the neural network to predict the critical disorder strength of a phase change from delocalized to localized. The reduced density matrix is defined in equation \ref{red_density}. Physically, the reduced density matrix $\rho_A$, provides correct measurement statistics for subsystem A.

\begin{eqnarray}
\rho_{AB}&=&\ket{\psi_A}\bra{\psi_A}\otimes\ket{\psi_B}\bra{\psi_B}\\
\rho_A&=&\text{Tr}_B(\rho_{AB})=\ket{\psi_A}\bra{\psi_A}\text{Tr}\left(\ket{\psi_B}\bra{\psi_B}\right)\label{red_density}
\end{eqnarray}%\url{http://www.thphys.nuim.ie/staff/jvala/Lecture_9.pdf}

The reduced density matrix was also used by Zhang in 2019 to learn the localization transition in disordered quantum Ising spin chains. Here, the motivation was to reduce the dimension and filter out redundant information. However, it proved to be inferior in comparison to the full density matrix in the analysis. \cite{Zhang2019} However, due to RAM limitations, we will rely on reduced density matrices.


\subsection{Artificial Neural Networks}

Rosenblatt published in 1958 his concept of the probabilistic model for information storage and organization in the brain, which greatly inspired others to use those models for computation.\cite{Rosenblatt1958} Over the course of years, they have evolved to a tool that can be used for a variety of applications including computer vision, speech recognition, medical diagnosis, playing games or even artistic painting.\cite{Gatys2015}

The reduced density matrices are essentially complex 2D arrays with length $2^n\times2^n$. As we want to classify for an arbitrary $W$ whether we have a localized or delocalized phase, it is convenient to use a machine learning classifier. The density matrices can then be thought of as a complex and real image that can be fed into it analogously to classical image classification.%todo: cnn


\section{Computational Methods}

The strategy for implementation was as follows:

\begin{enumerate}
	\item Generate Hamiltonian from random disorder strength and system size. Then calculate lowest eigenstate near Energy $E = 0$.
	\item Generate density matrix from the eigenstate and the respective reduced density matrices for defined block sizes $n$.
	\item  Set up machine learning model per $n$, $L$ that takes density matrices of different $W$ as an input and predicts whether the state represents an extended or a localized phase.
	\item Make predictions for different system sizes L and block sizes $n$ and plot the predictions over $W$. Then extract $W_c$	from the data by using a fit function.
\end{enumerate}

Critical decisions and specifications for each steps are listed below. Afterwards, a brief motivation for the parameter range and resolution is given.

\subsection{Eigenvalue solver}

For the eigenvalue solution, we use SciPy's method \texttt{eigsh}. However, the computation of eigenvalues near zero is computationally very costly and sometimes did not even converge during the standard maximum of iterations. 

Most ARPACK functionalities are included in \texttt{eigsh}. To compute the eigenvalues, we are relying on the so called shift-invert method, which is a mode that allows a quick determination of non-external eigenvalues. This mode involves transforming the eigenvalue problem to an equivalent problem with different eigenvalues. 

In particular, the method is based on the observation that one can find for the generalized eigenvalue problem

\begin{equation}
Ax=\lambda M x
\end{equation}
that
\begin{equation}
\left(A-\sigma M\right)^{-1}Mx = vx, \quad v=\frac{1}{\lambda-\sigma}.
\end{equation}

As we want to find the ground state, our choice for $\sigma$ is zero. The transformed eigenvalues will then satisfy $v=1/\lambda$, so our small eigenvalues  become large eigenvalues for which the Lanczos algorithm converges faster.\cite{Virtanen_2020} This method is also used as part of a efficient phase characterization method by Luitz in 2015 \cite{Luitz2015}.

\subsection{Computation of the reduced density matrix}

To get the reduced density matrix of system A, one has to "trace out" all states outside of A. The library QuTiP supplies a method \texttt{ptrace}, which does exactly that. It is important to note that the method takes those indices as an argument which should be kept.\cite{Johansson2012}

A demonstration of the functionality can be found in Figure \ref{fig:partialtrace_proof_of_concept}.

\begin{figure}[h!]
\centering
\includegraphics[width=0.7\linewidth]{figures/partialtrace_proof_of_concept}
\caption{Proof of concept for partial trace calculation similar to \protect\hyperlink{http://qutip.org/docs/3.1.0/guide/guide-tensor.html}{QuTiP-Guide/ptrace}.}
\label{fig:partialtrace_proof_of_concept}
\end{figure}

The algorithm of selecting the position vector of $n$ consecutive sites was implemented as follows: 
\begin{enumerate}
	\item Find the center spin rounded to next lowest integer.
	\item Determine left chain length $n_\text{left}$ as $n/2$ rounded to the next lowest integer.
	\item Determine right chain length $n_\text{right}$ as $n-n_\text{left}$.
	\item Select spins from left chain end to right chain end around center spin.
\end{enumerate}
This results in a behavior that picks left indices more favorably, but succeeds if equally spaced ends exist. Let the spins be numbered as $\{1, 2, 3, 4, 5\}$ for $N=5$, then  $n=3$ results in $\{2, 3, 4\}$, whereas $n=2$ results in $\{2, 3\}$.

These lattice sites serve then as an input to the partial trace function, such that the density matrix represents the measurement statistics of the center system.

\subsection{Machine learning models and error metrics}\label{sec:nn}

The decision for the machine learning framework \texttt{keras} was motivated by its flexibility and simplicity. \cite{chollet2015keras}

When setting up the machine learning model, one can already specify the first and last layer: The first (input) layer has to match the sample size of the incoming data, this can be already computed in advance. The length $len$ for block size $n$ is $2\cdot\left(2^n\times 2^n \right)$. The factor 2 comes from a preprocessing step, where the complex values are mapped to a second real picture, since the fitting procedure usually does not expect complex numbers. The last layer is a one node sigmoid, as the target output is the one-dimensional classification in $\left[0,1\right]$.

For small sample sizes, there exist various approaches to choose the right amount of layers and regularization methods \cite{Olson2018,Feng2019}, which cannot be generalized, as they heavily depend on feature size and target dimension. 

To balance the trade off between overfitting and loss, the starting point for the model was one hidden layer with 64 nodes. Since the reduced density matrices are similar to image classification and the inspection of the training set indicated that the density matrices had different slopes, a convolutional layer was employed for block sizes of $n<3$, as a $8x8$ picture seemed too small for kernel operations. To compensate the lacking layer, a dense layer of 32 nodes was chosen.
%todo: dropout for regularization

The optimizer Adam was chosen, because it is computationally efficient and
has little memory requirements. \cite{Kingma2014}

For a two label classification problem, it is useful to use cross-entropy as a loss metric, as the penalty increases exponentially the further one deviates from the correct prediction.\cite{Goodfellow-et-al-2016}
\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figures/cross_entropy}
\caption{Example for cross-entropy loss depending on the predicted probability of $\hat{y}=1$.}
\label{fig:cross_entropy}
\end{figure}
The definition for a two class cross-entropy loss can be found in equation \ref{cross-loss}, where $y \in \{0,1\}$ is the true class and $\hat{y}\in\left[0,1\right]$ the predicted probability. This loss is also plotted in Figure \ref{fig:cross_entropy}.
\begin{equation}
L(\hat{y}, y)=-\left(y\log(\hat{y})+(1-y)\log(1-\hat{y})\right)\label{cross-loss}
\end{equation}

\subsection{Extraction of critical disorder strength $W_c$}\label{sec:wcextract}

To fit for the critical disorder strength $W_c$, two functions were compared. The logistic Fermi-Dirac like function:
\begin{align}
\text{L} \colon \;\mathbb {R} &\to \left[0,1\right]\\
\ W_{pred}&\mapsto \frac{1}{\exp\left(-\alpha \left(W_{pred}-W_c\right)\right)+1}
\end{align}
and the heaviside function:
\begin{align}
\text{H} \colon \;
\mathbb {R} &\to \{0,1\}\\
\ W_{pred}&\mapsto 
\begin{cases}
0:&W_{pred}<W_c\\1:&W_{pred}\geq W_c
\end{cases}
	\end{align}
The fully delocalized phase is defined as $0$ and fully localized as $1$. Whereas the heaviside function has an abrupt step and only maps to the extrema, the logistic function serves as a smoother option for a transition, depending on the parameter $alpha$. The motivation came also from an optimizers view: Differentiable functions are easier to fit for the computer.\cite{Kyurkchiev2015} Therefore, the logistic function was used to extract $W_c$ with the empiric decision of $\alpha=50$.

\subsection{Limitations for parameter range and resolution}\label{sec:param}

\begin{enumerate}
	\item System size $L$: Limited by computing time of eigenvalue solver. For the system size $L=12$, one calculation lasted approximately two minutes.
	\item Block size $n$: We go up to $n=6$, which is half of the system size of the biggest system.
	\item Sample size: 1200 samples can be generated for $L=12$, $n_{max}=6$ in approximately 15 hours on the provided machine. Assuming that the whole program should be reproducable in a reasonable time frame, this was found to be a sufficient sample size per system and block size.
	\item Disorder strength $W$ for the testing set: Since each point of a test set comes with a Hamiltonian with randomly drawn $h_i\in\left[-W,W\right]$, a decent amount of variance can be expected for the phase prediction. As we want to extract the phase change in general, and are not interested in the particular phase predictions of one specific Hamiltonian we choose to regularize the prediction by averaging over $60$ predicted samples.
\end{enumerate}%todo: how many hours for L=12, n=7?

\section{Results}

\subsection{Generation of reduced density matrix training set}

The parameter range for the computation of the reduced density matrices can be found in Table \ref{tab:par_train}. The total computation time was 16.5 h, where 12.5 h where solely needed to compute the ground states of the $L=12$ system.

\begin{table}[h!]
	\centering
	\begin{tabular}{rl}
		\hline
		Parameter & Range or Set \\
		\hline
		\hline 
	\textbf{System size:} & $L \in \{9, 10, 11, 12\}$ \\ 
		\textbf{Block size:} & $n \in \{1, 2, 3, 4, 5, 6\}$ \\ 
		\textbf{Repetitions:} & $r=500$\\
		\hline
	\end{tabular} 
	\caption{Parameter choice for training set generation}\label{tab:par_train}
\end{table}%todo: Calculate/Plot Entanglement Entropy

In order to give some visual intuition, the Figures \ref{fig:groundstates_loc} and \ref{fig:groundstates_erg} show realizations for different block sizes and phases.

%\onecolumngrid
\begin{center}
	\begin{figure}[h!]
		\centering	
		\begin{subfigure}[c]{0.45\textwidth}
			\includegraphics[width=\linewidth]{../results/groundstates/N10n6_trainingset_groundstate_Wmax8.0}
			\subcaption{Visualization of the ground state for a large block size $n=6$ in the localized phase.}
		\end{subfigure}
		\begin{subfigure}[c]{0.45\textwidth}
			\includegraphics[width=\linewidth]{../results/groundstates/N10n4_trainingset_groundstate_Wmax8.0}
			\subcaption{Visualization of the ground state for an intermediate block size $n=4$ in the localized phase.}
		\end{subfigure}
		\begin{subfigure}[c]{0.45\textwidth}
			\includegraphics[width=\linewidth]{../results/groundstates/N10n1_trainingset_groundstate_Wmax8.0}
			\subcaption{Visualization of the ground state for the minimal block size $n=1$ in the localized phase.}
		\end{subfigure}
		\caption{Ground states for different block sizes $n$ and phases.}
		\label{fig:groundstates_loc}
\end{figure}
\end{center}
\begin{center}
	\begin{figure}[h!]
		\begin{subfigure}[c]{0.45\textwidth}
			\includegraphics[width=\linewidth]{../results/groundstates/N10n6_trainingset_groundstate_Wmax0.5}
			\subcaption{Visualization of the ground state for a large block size $n=6$ in the ergodic phase.}
		\end{subfigure}
		\begin{subfigure}[c]{0.45\textwidth}
			\includegraphics[width=\linewidth]{../results/groundstates/N10n4_trainingset_groundstate_Wmax0.5}
			\subcaption{Visualization of the ground state for an intermediate block size $n=4$ in the ergodic phase.}
		\end{subfigure}
		\begin{subfigure}[c]{0.45\textwidth}
			\includegraphics[width=\linewidth]{../results/groundstates/N10n1_trainingset_groundstate_Wmax0.5}
			\subcaption{Visualization of the ground state for the minimal block size $n=1$ in the ergodic phase.}
		\end{subfigure}
		\caption{Ground states for different block sizes $n$ and phases.}
		\label{fig:groundstates_erg}
	\end{figure}
\end{center}
%\twocolumngrid

The visual inspection indicates that the density matrix of the localized phase has a sharp maximum at the preferred state that is forced by the random disorder strength. The extended phase shows a sparse but certainly more even distribution, which reflects that still some configurations are more preferred than others induced by the coupling term in the Hamiltonian. The biggest maxima are likely still influenced by the random disorder strength. Another observation is that the density matrix reductions of the full ground state conserved these properties for $n>2$, when comparing $n=6$ to $n=\left[2,5\right]$. The similarity between the two phases gets smaller the smaller the block size $n$. For $n=1$, one could argue that the density matrices are very similar, as they only deviate for half of the matrix elements. In conclusion, the ergodic reduced density matrix shows far more thermalization than the localized one.

\subsection{Model training}\label{sec:loss_acc}

Before we can predict the phase of a newly generated test set, we have to train the neural network with our available training data. For each system and block size a separate model was trained, as a different system size might influence the physical behavior due to open boundary conditions.

The neural networks are generated as a sequential keras model with the following configuration, as discussed in section \ref{sec:nn}: 
%todo: update to final model
\begin{lstlisting}[language=Python]
model = models.Sequential()
if self.n > 3:
	filters = self.n*self.n
	model.add(layers.Conv2D(filters, (3, 3), activation='relu', input_shape=(np.shape(self.X_train[0])[0], np.shape(self.X_train[0])[1], 2)))
	model.add(layers.MaxPooling2D((4, 4)))
	model.add(layers.Flatten())
else:
	model.add(layers.Flatten(input_shape=(np.shape(self.X_train)[1], np.shape(self.X_train)[1], 2)))
	model.add(layers.Dense(32, activation='relu')),

model.add(layers.Dense(64, activation='relu', bias_regularizer='l2'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
\end{lstlisting}

Two strategies are employed to prevent over-fitting:
\begin{enumerate}
	\item $30$ \% of the training set was used for validation. To avoid a biased split, we relied on \texttt{sklearn}'s method \texttt{train\_test\_split} that samples randomly from the training set.
	%todo: Drop out layers?
	\item For regularization, a dropout layer was introduced with a dropout rate of $30$ \%.
	%todo: outcome?
\end{enumerate}

The model training was executed by using a batch size of 70 and 200 epochs, where the batch size was limited by the CPU performance and no significant loss or accuracy improvements where noted after 120 epochs.%todo: epoch number correct?

An example of the accuracy and loss dependency on the number of epochs for system size $L=10$, and block sizes $n=\{1, 6\}$ is presented below in Figure \ref{fig:N10n4_accuracy_loss_epochs}.
%todo: more training epochs

\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{../results/accuracy_loss_epochs/N10n4_accuracy_loss_epochs}
	\caption{Accuracy and loss evolution over training epochs.}
	\label{fig:N10n4_accuracy_loss_epochs}
\end{figure}
%todo: describe overfitting

Figure \ref{fig:N10n4_accuracy_loss_epochs} illustrates that the model is still not completely prone to overfitting. This can be accounted towards the big model size in comparison to the sample size. Still, the accuracy reaches an acceptable optimum after 100 training epochs.%todo: number epochs

\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{../results/accuracy_loss_epochs/all_validation_accuracy}
	\caption{Overview over the resulting accuracies on the validation set.}
	\label{fig:all_validation_accuracy}
\end{figure}

The validation accuracy was highest for large system sizes and large block sizes. The reason for that is probably that most entries of the density matrices are zero, which makes a bigger impact on the network for larger block sizes and therefore density matrices.
%todo: compare with entaglement entropy
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{../results/accuracy_loss_epochs/all_validation_losses}
	\caption{Overview over the resulting losses on the validation set.}
	\label{fig:all_validation_accuracy}
\end{figure}

The validation losses in Figure \ref{fig:all_validation_accuracy} tell a similar story. Here, intermediate block sizes of $n=\{3, 4\}$ show the lowest validation loss. In the intermediate regime, the density matrices have a reasonable trade off between level of detail and number of weights in the neural network, since the first layer must match the input dimensions.

In conclusion, large system sizes and intermediate block sizes showed the best results. In addition, one can safely disregard predictions by models for block size $n=1$.

\subsection{Analysis of critical disorder strength}
\subsubsection{Dependency on block size}
First, the testing set was generated. Following the parameter discussion in section \ref{sec:param}, we generate five samples for each $W\in\left[0,4\right]$, with step $\Delta W=0.05$, resulting in 400 samples per system and block size. Afterwards, the predictions were fitted with a logistic function to obtain $W_c$ as described in section \ref{sec:wcextract}.
Five predicted phases are averaged at each point and plotted to a heat map. The fitted $W_c$ is plotted along in Figure \ref{fig:wcextractn}.

\onecolumngrid
\begin{center}%todo: annotate colorbar
	\begin{figure}[H]
		\centering	
		\begin{subfigure}[c]{0.45\textwidth}
			\includegraphics[width=\textwidth]{../results/Wc/N8_Wc_n_dependency.pdf}
			\subcaption{$L=8$}
		\end{subfigure}
		\begin{subfigure}[c]{0.45\textwidth}
			\includegraphics[width=\textwidth]{../results/Wc/N9_Wc_n_dependency.pdf}
			\subcaption{$L=9$}
		\end{subfigure}
		\begin{subfigure}[c]{0.45\textwidth}
			\includegraphics[width=\textwidth]{../results/Wc/N10_Wc_n_dependency.pdf}
			\subcaption{$L=10$}
		\end{subfigure}
		\begin{subfigure}[c]{0.45\textwidth}
			\includegraphics[width=\textwidth]{../results/Wc/N11_Wc_n_dependency.pdf}
			\subcaption{$L=11$}
		\end{subfigure}
		\begin{subfigure}[c]{0.45\textwidth}
			\includegraphics[width=\textwidth]{../results/Wc/N12_Wc_n_dependency.pdf}
			\subcaption{$L=12$}
		\end{subfigure}
		\caption{Dependency of the phase transition on block size $n$ for different system sizes.}
		\label{fig:wcextractn}
	\end{figure}
\end{center}
\twocolumngrid

In conclusion, the predicted critical disorder strength $W_c$ decayed, when models with larger block sizes $n$ were used for prediction. The low $W_c$ values for $n=\{1,2\}$ might just as well be attributed to the poor loss and accuracy values shown in section \ref{sec:loss_acc}. An explanation for this decay might be that a bigger block size can more accurately reflect the level of disorder forced on the system. For smaller block sizes, for some spins the information is lost whether the configuration was the result of interacting lattice sites or the random disorder strength.

\subsubsection{Dependency on system size}

%todo: look at entropy plots of paper and replicate them!

something something%todo: write


\onecolumngrid
\begin{center}
	\begin{figure}[H]
		\centering	
		\begin{subfigure}[c]{0.4\textwidth}
			\includegraphics[width=\textwidth]{../results/Wc/n1_Wc_N_dependency.pdf}
			\subcaption{$n=1$}
		\end{subfigure}
		\begin{subfigure}[c]{0.4\textwidth}
			\includegraphics[width=\textwidth]{../results/Wc/n2_Wc_N_dependency.pdf}
			\subcaption{$n=2$}
		\end{subfigure}
		\begin{subfigure}[c]{0.4\textwidth}
			\includegraphics[width=\textwidth]{../results/Wc/n3_Wc_N_dependency.pdf}
			\subcaption{$n=3$}
		\end{subfigure}
		\begin{subfigure}[c]{0.4\textwidth}
			\includegraphics[width=\textwidth]{../results/Wc/n4_Wc_N_dependency.pdf}
			\subcaption{$n=4$}
		\end{subfigure}
		\begin{subfigure}[c]{0.4\textwidth}
			\includegraphics[width=\textwidth]{../results/Wc/n5_Wc_N_dependency.pdf}
			\subcaption{$n=5$}
		\end{subfigure}
		\begin{subfigure}[c]{0.4\textwidth}
			\includegraphics[width=\textwidth]{../results/Wc/n6_Wc_N_dependency.pdf}
			\subcaption{$n=6$}
		\end{subfigure}
		\caption{Dependency of the phase transition over system sizes $L$ for different block sizes $n$.}
		\label{fig:wcextract}
	\end{figure}
\end{center}
\twocolumngrid%todo: compare with introduction

%W—>0 : reduced matrix : thermal density matrix , volume law entanglement
%W —> \infty : reduced density matrix: … , area law entanglement
%n is finite and large enough
%reduced density matrix would be different on two sides.
%todo: Finite Scaling


%todo: Plot N dependency
The plots are indicating that a bigger system size requires a larger disorder strength to perform the phase transition.


\section{Conclusion}

$W_c$ depends on n, L (yes/no).

$W_c$ prediction coincides with the expectation (yes/no)

$W_c$ is dependent on these and that effects => scaling analysis? (yes/no)
%todo: write conclusion

\bibliography{zotero}
%\bibliography{bibsamp}% Produces the bibliography via BibTeX.

\newpage
\appendix


\begin{widetext}
\section{Code listing} \label{app:codes}
%todo: add github permanent link
The process consists essentially of five different steps, which can be called through a main function, but can also be run separately. Every file serves a number of different purposes as listed below.%todo: add flowchart and folder structure

\begin{enumerate}
	\item \textbf{generate\_training\_set.py}: Here, the training set is generated and some example plots of ground states are saved to the results folder. The training sets are saved in the \texttt{training\_sets} folder, where they are numbered with their system and block size.
	\item \textbf{ed.py}: The training set is generated by using the functions from the tutorial. A new function was added that generates the Hamiltonian using the random local disorder strength.
	\item \textbf{dataset\_preparation.py}: While this file contains many important functions to preprocess and label the training and testing sets and load and save functions, it also has a method that plots the visualizations of a few ground states.
	\item \textbf{model\_save\_train.py}: First, models are generated that automatically match the input data of different block sizes $n$, afterwards, they are trained with a certain amount of epochs and batch sizes. The history of the validation and accuracy is plotted individually into the results folder.
	\item \textbf{generate\_test\_set.py}: A set of reduced density matrices for ground states in the intermediate regime is generated.
	%todo: plot state near W=1
	\item \textbf{generate\_predictions.py}: The test sets are fed into the trained machine learning models. The predicted phases are averaged for every W, n, N combination and saved into a prediction dataset.
	\item \textbf{plot\_wc\_dependency.py}: The predictions are loaded, we extract $W_c$ and plot everything together as a heat map over system and block sizes.
\end{enumerate}

\subsection{Training set generation}
\lstinputlisting[language=Python]{../generate_training_set.py}

\subsection{Exact diagonalization}
\lstinputlisting[language=Python]{../ed.py}

\subsection{Dataset Preparation}
\lstinputlisting[language=Python]{../dataset_preparation.py}

\subsection{Model Training}
\lstinputlisting[language=Python]{../model_save_train.py}

\subsection{Test set generation}
\lstinputlisting[language=Python]{../generate_test_set.py}

\subsection{Prediction}
\lstinputlisting[language=Python]{../generate_predictions.py}

\subsection{Evaluation of $W_c$}
\lstinputlisting[language=Python]{../plot_wc_dependency.py}



%\begin{lstlisting}[language=Python]
%%\include{file}
%\include{}
%\end{lstlisting}
\end{widetext}



\end{document}
